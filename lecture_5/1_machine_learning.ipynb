{
 "metadata": {
  "name": "",
  "signature": "sha256:03a28ea1164d6cf80be60664f60c92642a04ddaa22bc1ad246e9a51c5d4b4288"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Ajuste de curvas"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from IPython.html.widgets import interact, fixed\n",
      "from IPython.html import widgets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El problema cl\u00e1sico de ajuste de curvas es dado un conjunto de puntos, encontrar la curva que **mejor** representa los datos. Donde **mejor** est\u00e1 sujeto a definici\u00f3n."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Para hacerlo m\u00e1s divertido, ver\u00e9mos el problema de ajuste de curvas como un problema de optimizaci\u00f3n y en particular como un m\u00e9todo de aprendizaje de m\u00e1quina (_machine learning_) supervisado."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Supongamos que tenemos un conjunto $m$ de datos de entrada $\\textbf{x}$ (en aprendizaje de m\u00e1quina se les conoce como _features_) y sus datos dependientes, $\\textbf{y}$ (_target_ en aprendizaje de m\u00e1quina). A este conjunto $(x^{(i)}, y^{(i)})$, le llamamos conjunto de entrenamiento. Queremos desarrollar un modelo $\\hat{\\textbf{y}}$ que aproxime el valor de $\\textbf{y}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "La primera aproximaci\u00f3n es intentar una aproximaci\u00f3n de forma lineal, conocida como _regresi\u00f3n lineal_, de la forma\n",
      "\n",
      "$$\n",
      "\\hat{\\textbf{y}} = \\mathbf\\beta_0 + \\mathbf\\beta_1 \\textbf{x}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Donde (si recuerdan Geometr\u00eda Anal\u00edtica) $\\beta_0$ es el _interceptor_ de la recta $\\hat{\\textbf{y}}$ y $\\beta_1$ es la _pendiente_ de la recta. A $\\hat{\\textbf{y}}$ se le conoce tambi\u00e9n como _hip\u00f3tesis_ y se le puede denotar con la variable $\\textbf{h}$. Si definimos que $x_0 \\equiv 1$, podemos escribir la _hip\u00f3tesis_ de manera m\u00e1s compacta:\n",
      "\n",
      "$$\n",
      "\\hat{\\textbf{y}} = \\sum_{j=0}^n \\beta_j x_j = \\mathbf{\\beta}^T\\textbf{x}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Con esta notaci\u00f3n, podemos extender el formalismo a m\u00e1s dimensiones (en este caso $n$).\n",
      "\n",
      "Una posible definici\u00f3n de **mejor** es que el modelo (el cual est\u00e1 determinado por $\\vec\\beta$) minimice la suma de las diferencias entre el valor actual $\\textbf{y}$ y el predicho $\\hat{\\textbf{y}}$ (a esta diferencia se le conoce como _error en la predicci\u00f3n_), en otras palabras _minimizar la suma del cuadrado de los residuos_. La funci\u00f3n a minimizar se conoce en aprendizaje de m\u00e1quina como _funci\u00f3n de costo_ $\\textbf{J}$. Debido a que tenemos varios pares $(x_i, y_i)$, la funci\u00f3n costo a minimizar es el _error cuadr\u00e1tico promedio_."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\textbf{J}(\\beta_0, \\beta_1) = \\frac{1}{2n}\\sum_{(x^{(i)}, y^{(i)}) \\in X \\times Y} (y^{(i)} - \\hat{y}^{(i)}(x^{(i)}))^2 = \\frac{1}{2n}\\sum_{(x^{(i)}, y^{(i)}) \\in X \\times Y} (y^{(i)} - \\beta_0 - \\beta_1 x^{(i)})^2$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El factor de $2$ se agrega para simplificar c\u00e1lculos posteriores. Visto as\u00ed, el objetivo de un algoritmo supervisado de aprendizaje de m\u00e1quina es encontrar $\\beta_0$ y $\\beta_1$ que minimiza la funci\u00f3n de costo $\\textbf{J}(\\beta_0, \\beta_1)$. Esto se puede hacer mediante un algoritmo llamado _gradient descent_."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Gradient descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El gradiente de una funci\u00f3n $g(x,y)$ es:\n",
      "\n",
      "$$\n",
      "\\nabla g (x, y) = \\left [ \\partial_x g, \\ \\partial_y g\\right]\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "donde $\\partial_x$, $\\partial_y$ es la derivada parcial respecto a $x$ y $y$ respectivamente. el significado geom\u00e9trico del gradiente de una funci\u00f3n, es el vector que apunta en la direcci\u00f3n donde se maximiza el incremento de la funci\u00f3n. Por lo tanto, si queremos minimizar la funci\u00f3n, recorremos el vector en el sentido contrario."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Algoritmo"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Escoger un punto al azar, llama a este punto $\\boldsymbol{B}_0$.\n",
      "2. Calcular el gradiente de $\\textbf{J}$ en esa locaci\u00f3n.\n",
      "3. Actualiza la locaci\u00f3n en el sentido opuesto a donde apunte el gradiente, espec\u00edficamente resta a $\\boldsymbol{B}_0$ el valor de $\\alpha\\nabla\\textbf{J}$, donde $\\alpha$ es un n\u00famero peque\u00f1o, conocido como _learning rate_.\n",
      "4. Repite los pasos $2$ y $3$ cuantas veces sea necesario."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En pseudoc\u00f3digo:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-warning\">\n",
      "_repetir hasta que lograr convergencia {_\n",
      "\n",
      "$$\\beta_j := \\beta_j - \\alpha \\frac{\\partial}{\\partial \\beta_j} \\textbf{J}(\\beta)$$\n",
      "\n",
      "_}_\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Se deja como ejercicio de tarea, demostrar que \n",
      "\n",
      "$$\n",
      "\\frac{\\partial}{\\partial \\beta_j} \\textbf{J}(\\beta) = \\frac{1}{m}\\sum_{i=1}^{m}\\left(\\hat{y}(x^{(i)}) - y(x^{(i)})\\right) \\cdot x^{(i)}_j\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Entonces:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-warning\">\n",
      "_repetir hasta que lograr convergencia {_\n",
      "\n",
      "$$\n",
      "\\beta_j := \\beta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\hat{y}(x^{(i)}) - y(x^{(i)})\\right) \\cdot x^{(i)}_j\n",
      "$$\n",
      "\n",
      "_}_\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "class RegresionLineal:\n",
      "    def __init__(self, alpha=0.3, max_iters=100, tols=0.001):\n",
      "        \"\"\"\n",
      "        Par\u00e1metros.\n",
      "        ---------------\n",
      "        alpha = Learning rate\n",
      "        max_iters = N\u00famero m\u00e1ximo de iteraciones\n",
      "        tols = definici\u00f3n de convergencia\n",
      "        \"\"\"\n",
      "        self.alpha = alpha\n",
      "        self.max_iters = max_iters\n",
      "        self.tols = tols\n",
      "        self.breaking_iteration = None\n",
      "        self.historia = {'costo':[], 'beta':[]}  # Con fines de graficaci\u00f3n\n",
      "        \n",
      "    def gradientDescent(self, x, y):\n",
      "        \"\"\"\n",
      "        Par\u00e1metros:\n",
      "        ---------------\n",
      "        x = vector de entrenamiento de features\n",
      "        y = vector de entrenamiento de variable a predecir (target)\n",
      "        \"\"\"    \n",
      "        \n",
      "        # ajustamos el vector de features\n",
      "        unos = np.ones((x.shape[0], 1))\n",
      "        Xt = X.reshape(x.shape[0], 1)\n",
      "        Xt = np.concatenate((unos, Xt), axis=1)\n",
      "        \n",
      "        i = 0\n",
      "        prep_J = 0\n",
      "        m, n = Xt.shape\n",
      "        self.beta = np.zeros(n) \n",
      "        \n",
      "        while i < self.max_iters:     \n",
      "            # Actualizamos beta\n",
      "            self.beta = self.beta - self.alpha * self.gradiente(Xt, y)\n",
      "            \n",
      "            J = self.costo(Xt, y)\n",
      "            \n",
      "            if abs(J - prep_J) <= self.tols:\n",
      "                print 'La funci\u00f3n convergi\u00f3 con beta: %s en la iteraci\u00f3n %i' % ( str(self.beta), i )\n",
      "                self.breaking_iteration = i\n",
      "                break\n",
      "            else:\n",
      "                prep_J = J\n",
      "            \n",
      "            self.historia['costo'].append(J)\n",
      "            self.historia['beta'].append(self.beta)                \n",
      "            i += 1\n",
      "    \n",
      "    def hipotesis(self, x):\n",
      "        return np.dot(x, self.beta)\n",
      "    \n",
      "    def costo(self, x, y):\n",
      "        m = x.shape[0]\n",
      "        error = self.hipotesis(x) - y\n",
      "        return np.dot(error.T, error) / (2 * m) \n",
      "    \n",
      "    def gradiente(self, x, y):\n",
      "        m = x.shape[0]\n",
      "        error = self.hipotesis(x) - y        \n",
      "        return np.dot(x.T, error) / m    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Ejemplo"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Los archivos `edad.dat` y `altura.dat` contienen las mediciones de las estaturas (en metros) de varios ni\u00f1os entre las edad de 2 y 8 a\u00f1os. Cada _tupla_ de altura y edad, constituyen un ejemplo de entrenamiento $(x^{(i)}, y^{(i)})$ de nuestros datos. Hay $m = 50$ datos para entrenar que usaremos para realizar un modelo de regresi\u00f3n lineal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.loadtxt('data/edad.dat')\n",
      "Y = np.loadtxt('data/altura.dat')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Los datos se ven as\u00ed:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(X,Y, label=\"data\")\n",
      "plt.xlabel('Edad (yr)')\n",
      "plt.ylabel('Altura (m)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-info\">\n",
      "\n",
      "**Ejercicio:** Usando el _widget_ siguiente, trata de encontrar al \"tanteo\" cu\u00e1l es la recta que mejor minimiza el error.\n",
      "\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plotGuess(x, y, interceptor, pendiente):\n",
      "    \n",
      "    modelo = lambda x,b,m: b + m*x # funci\u00f3n para graficar el modelo\n",
      "    \n",
      "    plt.scatter(X,Y, label=\"data\")\n",
      "    plt.plot(X, modelo(X, interceptor, pendiente), label='Guess')\n",
      "    plt.xlabel('Edad (yr)')\n",
      "    plt.ylabel('Altura (m)')\n",
      "    plt.legend(loc=\"best\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interact(plotGuess, x=fixed(X), y=fixed(Y), interceptor=(0,2,0.02), pendiente=(0,2, 0.02));"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Entrenamos la regresi\u00f3n lineal con un _learning rate_ de $\\alpha = 0.03$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = RegresionLineal(alpha=0.03, max_iters=10000, tols=0.0000001)\n",
      "r.gradientDescent(X, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-info\">\n",
      "**Ejercicio: ** Agrega un _widget_ de interacci\u00f3n en el cual puedas modificar $\\alpha$. \u00bfQu\u00e9 observas?\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plotModelo(x,y,rl,iteracion):\n",
      "    modelo = lambda x,b,m: b + m*x # funci\u00f3n para graficar el modelo\n",
      "    \n",
      "    _beta = rl.historia['beta'][iteracion]\n",
      "\n",
      "    fig, ax = plt.subplots(1,2, figsize=(10,6))\n",
      "    ax[0].scatter(x,y, label=\"datos\")\n",
      "    ax[0].plot(x, modelo(x, _beta[0], _beta[1]), label=\"int: %1.2f, pen: %1.2f\" % (_beta[0], _beta[1]))\n",
      "    ax[0].set_xlabel('Edad (yr)')\n",
      "    ax[0].set_ylabel('Altura (m)')\n",
      "    ax[0].legend(loc=\"best\")\n",
      "    #ax[0].set_xlim(0, max(x))\n",
      "    #ax[0].set_ylim(0, max(y))\n",
      "    \n",
      "    costo  = rl.historia['costo']\n",
      "    \n",
      "    iteraciones = [i for i in range(0, len(costo))]\n",
      "    ax[1].plot(iteraciones, costo, 'g', label=\"costo\")\n",
      "    ax[1].plot(iteracion, costo[iteracion], 'or', label=\"iteracion\")\n",
      "    ax[1].set_xlabel('Iteraciones')\n",
      "    ax[1].set_ylabel('Costo')\n",
      "    ax[1].legend(loc=\"best\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plotModelo(X,Y, r, 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ultima_iteracion = (r.breaking_iteration - 1) if r.breaking_iteration else (r.max_iters - 1)\n",
      "\n",
      "interact(plotModelo, x=fixed(X), y=fixed(Y), rl=fixed(r), iteracion=(0,ultima_iteracion,10));"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-info\">\n",
      "**Ejercicio**: Modifica el _widget_ que creaste, para que dibujes la \u00faltima iteraci\u00f3n \u00bfTienes una mejor intuici\u00f3n de $\\alpha$?\n",
      "</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Una cosa m\u00e1s..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-warning\">\n",
      "\n",
      "A pesar de que la regresi\u00f3n lineal (obtenida con el _gradient descent_) parece un algoritmo muy simple, los conceptos son los mismos que para algoritmos de aprendizaje de m\u00e1quina m\u00e1s avanzados, i.e. minimizar una funci\u00f3n de costo. Estos algoritmos simplemente reemplazan el modelo linear con un modelo m\u00e1s complejo (y con una funci\u00f3n de costo m\u00e1s compleja). De cierta manera, los algoritmos de aprendizaje de m\u00e1quina son problemas de optimizaci\u00f3n.\n",
      "\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}